{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Foundamentals of Data Science\n",
    "### Solar Eclipse phases classification\n",
    "#### Luca Moresca, Nicholas Suozzi, Valerio Santini\n",
    "\n",
    "---------\n",
    "-   **Titolo del progetto:** Classificazione delle fasi dell'eclissi solare \n",
    "-   **Autori:** Luca Moresca, Nicholas Suozzi, Valerio Santini\n",
    "-   **Descrizione del problema:** Fornire una breve panoramica del problema della classificazione delle fasi dell'eclissi solare. Evidenziare l'importanza dello studio delle eclissi solari per la comprensione dell'interazione tra Sole e Terra.\n",
    "-   **Obiettivo del progetto:** Definire chiaramente l'obiettivo del progetto, ovvero sviluppare un modello in grado di classificare accuratamente le diverse fasi di un'eclissi solare da immagini.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    %pip install -qqq numpy scipy matplotlib pandas scikit-learn seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from PIL import Image\n",
    "from scipy.signal import convolve2d\n",
    "import math\n",
    "from scipy.ndimage import convolve1d\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.transform import resize\n",
    "from skimage import io, color\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.draw import rectangle\n",
    "from skimage.filters import threshold_otsu\n",
    "from typing import List, Tuple, Optional, Dict, Union\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# sets up pandas table display\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The “Great American Eclipse” in 2017 offered a unique opportunity for US citizens across the entire continent to experience the awe-inspiring changes that occur during rare total solar eclipses (TSEs). The goal of the 2017 Eclipse Megamovie was to obtain enough pictures of the Great American Total Solar Eclipse to create a movie of the solar corona in order to better study it. Over 2,000 volunteers submitted 50,000 images and a movie was made only a few hours after the Moon's shadow left the U.S. A few months later another, better aligned and more closely edited, movie was released. The Eclipse Megamovie team of scientists processed images to create High Dynamic Range (HDR) images to capture the structures within the high dynamic range of the solar corona from the Sun's photosphere out ~3 solar radii.\n",
    "https://eclipsemegamovie.org\n",
    "\n",
    "Eclipse megamovie is a project being carried out by a group of researchers in collaboration with NASA. The focus of this project is to make a video that can facilitate research into a rare event such as the eclipse that occurred in the United States in 2017. To realise this project, American citizens were asked to send in photographs taken during the eclipse. The dataset therefore contains semi-professional and amateur images, taken under conditions that are not always optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the .json file with the label.\n",
    "with open('eclipse-megamovie/label_num_to_phase_map.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "print(json.dumps(data, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the .csv file with labels and images names.\n",
    "df = pd.read_csv(\"eclipse-megamovie/train.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .json file relates the eclipse phases to the corresponding labels in the ‘train.csv’ file, which will help the training phase by already having a correspondence between label and image. The csv file then contains a classification of the various training images useful for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of some images in the train dataset\n",
    "\n",
    "img1 = mpimg.imread('eclipse-megamovie/train/00a0f91f0e7ece88ebaeab2a2050cdc0d72c5ea0b56c601dfaf800077df44c51.jpg')\n",
    "img2 = mpimg.imread('eclipse-megamovie/train/00a4af4cc3f2a667c80331051a733e8dae2477d682d7dcef3e895f6f923dd86a.jpg')\n",
    "img3 = mpimg.imread('eclipse-megamovie/train/00c11b16fd6426a88039c8b27e2511e2e770b65e0eb9daab720d3e6ca996bc06.jpg')\n",
    "img4 = mpimg.imread('eclipse-megamovie/train/001e6704a60798684f1656713b593e6a56de81474e7173a31936589cd62f13e8.jpg')\n",
    "img5 = mpimg.imread('eclipse-megamovie/train/4f99a8e1efe0a51fca127bcdabfb1165523dee8730dbba0df5436a7de70dc16e.jpg')\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(16, 4))\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "axes[0].imshow(img1)\n",
    "axes[0].set_title(\"Image 1\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(img2)\n",
    "axes[1].set_title(\"Image 2\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(img3)\n",
    "axes[2].set_title(\"Image 3\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "axes[3].imshow(img4)\n",
    "axes[3].set_title(\"Image 4\")\n",
    "axes[3].axis('off')\n",
    "\n",
    "axes[4].imshow(img5)\n",
    "axes[4].set_title(\"Image 5\")\n",
    "axes[4].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As can be seen in the figure above, our dataset consists of various types of images. Some contain eclipses, others do not. The images have different perspectives, are taken with different devices and have different backgrounds. \n",
    "This presents us with the challenge of correctly identifying the content of each image accurately, we must be able to classify images that do not contain eclipses as impostors. \n",
    "On a numerical level, the dataset consists of a training set and a test set. In addition, there is a .csv file containing the name of some images and a label showing the corresponding eclipse phase. In addition, there is a .json file that explains what each label corresponds to on a phase level. \n",
    "There are 496 images in the training set and 140 images in the test set. The dataset is restricted and this will result in an appropriate treatment of the images to improve the performance of the model. \n",
    "\n",
    "* Each image has a different resolution, considering that they are obtained with various devices. The eclipse within each image is not always perfectly sharp or centred, making the model more elastic in various cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing delle immagini\n",
    "\n",
    "-   **Spiegazioni:** Descrivere le tecniche di preprocessing delle immagini utilizzate, spiegando la logica alla base di ogni scelta.\n",
    "-   Ad esempio, se si ridimensionano le immagini, spiegare perché è necessario farlo e quale dimensione si è scelto di utilizzare.\n",
    "-   Se si convertono le immagini in scala di grigi, giustificare questa scelta.\n",
    "-   Se si normalizzano i valori dei pixel, spiegare il metodo di normalizzazione utilizzato e perché è importante.\n",
    "-   **Codice:** Fornire il codice Python utilizzato per il preprocessing delle immagini.\n",
    "-   **Visualizzazioni:** Mostrare alcuni esempi di immagini pre-elaborate, confrontandole con le immagini originali.\n",
    "\n",
    "\n",
    "### Suggerimenti per il Preprocessing delle Immagini per la Classificazione delle Fasi di Eclissi Solare\n",
    "\n",
    "Ecco alcuni suggerimenti per descrivere le tecniche di preprocessing delle immagini che potresti utilizzare per il tuo progetto di classificazione delle fasi di eclissi solare, insieme alla logica alla base di ogni scelta:\n",
    "\n",
    "*   **Conversione in scala di grigi:** La conversione delle immagini in scala di grigi è una scelta comune nel preprocessing delle immagini, soprattutto quando il colore non è una caratteristica discriminante per il compito in questione. Nel caso della classificazione delle fasi di eclissi solare, le informazioni di colore potrebbero non essere essenziali per distinguere le diverse fasi. La conversione in scala di grigi riduce la complessità dei dati e la quantità di informazioni da elaborare, il che può accelerare i tempi di addestramento del modello e potenzialmente migliorare le sue prestazioni.\n",
    "*   **Riduzione del rumore:** Le immagini possono contenere rumore che può influenzare negativamente le prestazioni del modello. La riduzione del rumore può essere realizzata utilizzando filtri come il filtro gaussiano.  Applicare un filtro gaussiano prima di altre operazioni di preprocessing può aiutare a migliorare la qualità delle immagini e a rendere più evidenti le caratteristiche importanti per la classificazione.\n",
    "*   **Downscaling:**  Se le immagini originali hanno una risoluzione molto alta, il downscaling può essere utile per ridurre la quantità di dati da elaborare e accelerare i tempi di addestramento. La dimensione da utilizzare dipenderà dalla risoluzione originale delle immagini e dalla complessità del modello. È importante scegliere una dimensione che conservi le caratteristiche essenziali per la classificazione, evitando al contempo di perdere troppe informazioni.  Assicurati di applicare un filtro gaussiano prima del downscaling per evitare l'aliasing. \n",
    "*   **Normalizzazione:** La normalizzazione dei valori dei pixel è un'altra tecnica comune nel preprocessing delle immagini.  Può essere eseguita sottraendo la media e dividendo per la deviazione standard dei valori dei pixel. Questo processo porta tutti i valori dei pixel in un intervallo simile, il che può aiutare a migliorare le prestazioni del modello, soprattutto quando si utilizzano algoritmi di apprendimento automatico sensibili alla scala dei dati.\n",
    "\n",
    "**Tecniche utili dagli homework:**\n",
    "\n",
    "*   **Dal Homework 1:** Tutte le tecniche elencate (riduzione del rumore, downscaling, normalizzazione e conversione in scala di grigi) possono essere utili per il tuo progetto.\n",
    "*   **Dal Homework 2:** La codifica one-hot sarà necessaria per le etichette delle fasi dell'eclissi. La gestione dei valori mancanti potrebbe essere utile se il tuo set di dati contiene immagini incomplete o danneggiate. L'espansione polinomiale e lo scaling potrebbero non essere necessari per il tuo progetto, in quanto sono tecniche più comunemente utilizzate per dati numerici non immagine.\n",
    "\n",
    "### Tecniche Essenziali di Preprocessing per Immagini di Eclissi Solari Variabili\n",
    "\n",
    "Dato che si ha a che fare con una vasta gamma di immagini scattate da amatori, che presentano diverse proporzioni, dimensioni del sole e posizioni del sole all'interno dell'immagine, è fondamentale applicare tecniche di preprocessing essenziali per preparare i dati per l'addestramento del modello di classificazione delle fasi dell'eclissi. \n",
    "\n",
    "Oltre alle tecniche di base già discusse (conversione in scala di grigi, riduzione del rumore, downscaling e normalizzazione), ecco alcune altre tecniche molto essenziali da considerare:\n",
    "\n",
    "*   **Ritaglio e ridimensionamento:** Le immagini potrebbero avere bordi o aree irrilevanti che non contribuiscono alla classificazione delle fasi dell'eclissi. Il ritaglio può essere utilizzato per rimuovere queste aree e focalizzare l'attenzione sulla regione di interesse, ovvero il sole e la corona solare. Dopo il ritaglio, tutte le immagini dovrebbero essere ridimensionate a una dimensione uniforme per garantire la compatibilità con il modello. Poiché le proporzioni delle immagini originali possono variare, è possibile utilizzare un ridimensionamento con padding per mantenere le proporzioni originali, aggiungendo bordi neri attorno all'immagine ritagliata per raggiungere la dimensione desiderata. \n",
    "*   **Centratura del Sole:** La posizione del sole all'interno dell'immagine può variare notevolmente. Per migliorare le prestazioni del modello, è utile centrare il sole in tutte le immagini. Ciò può essere ottenuto tramite algoritmi di rilevamento degli oggetti o, in casi più semplici, analizzando l'istogramma dell'immagine per identificare la regione più luminosa (presumibilmente il sole) e spostarla al centro dell'immagine.\n",
    "\n",
    "### Come Rilevare e Centrare il Sole nelle Immagini di Eclissi\n",
    "\n",
    "Per centrare il sole in un'immagine, un metodo robusto è quello di utilizzare la **combinazione di rilevamento dei bordi, piramide gaussiana e template matching**, tecniche già presenti negli homework forniti.\n",
    "\n",
    "**Ecco i passaggi**\n",
    "\n",
    "1. **Rilevamento dei Bordi**\n",
    "\n",
    "2. **Piramide Gaussiana** \n",
    "\n",
    "3. **Template Matching** \n",
    "\n",
    "4. **Selezione della Scala Migliore** \n",
    "\n",
    "5. **Centratura** \n",
    "\n",
    "**Note Aggiuntive**\n",
    "\n",
    "* Per migliorare ulteriormente la precisione del rilevamento, è possibile utilizzare un template del sole più sofisticato, che tenga conto della corona solare.\n",
    "* Dopo la centratura, potrebbe essere necessario ritagliare l'immagine per rimuovere eventuali bordi neri aggiunti durante il ridimensionamento con padding.\n",
    "\n",
    "Questo metodo offre una soluzione efficace per rilevare e centrare il sole in immagini di eclissi solare, utilizzando tecniche già familiari dagli homework e garantendo una buona robustezza e flessibilità.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversione in scala di grigi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "path = \"eclipse-megamovie/train/f5b6b5e1fe97a2070a105c2031d1d84d4cc764746dc9d4c5a94db9b099d976a2.jpg\"\n",
    "#path = \"eclipse-megamovie/train/00a0f91f0e7ece88ebaeab2a2050cdc0d72c5ea0b56c601dfaf800077df44c51.jpg\"\n",
    "image = cv2.imread(path)\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# cv2.imwrite('/Users/valeriosantini/Desktop/or.jpg', image)\n",
    "# cv2.imwrite('/Users/valeriosantini/Desktop/gr.jpg', gray_image)\n",
    "\n",
    "plt.imshow(gray_image, cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMENTARE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss(sigma: float, filter_size: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    Gx = np.array([])\n",
    "    k = (filter_size - 1) // 2  \n",
    "    x = np.arange(-k, k + 1)\n",
    "    cost = 1/(np.sqrt(2*np.pi)*sigma)\n",
    "    Gx = np.append(Gx, cost * np.exp(-(x**2)/(2*sigma**2)))\n",
    "    return Gx, x\n",
    "\n",
    "def gaussfiltering(img: np.ndarray, sigma: float) -> np.ndarray:\n",
    "    filter_size = np.round(6 * sigma)\n",
    "\n",
    "    Gx, _ = gauss(sigma, filter_size)\n",
    "    out = convolve1d(img, Gx, axis = 1)\n",
    "    out = convolve1d(out, Gx, axis = 0)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 3\n",
    "\n",
    "smt_img = gaussfiltering(gray_image, sigma)\n",
    "\n",
    "plt.imshow(smt_img, cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMENTARE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(img):\n",
    "    return (img - img.min()) / (img.max() - img.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrm_img = normalize(smt_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalize function performs linear normalization of an image to scale pixel values in the range $[0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eclipse Isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_eclipse(image):\n",
    "\n",
    "\n",
    "    offset = 128\n",
    "    intensity_threshold = threshold_otsu(image)\n",
    "    bright_pixels = image > intensity_threshold\n",
    "\n",
    "    labeled_bright = label(bright_pixels)\n",
    "    regions = regionprops(labeled_bright)\n",
    "\n",
    "\n",
    "    if len(regions) == 0:\n",
    "        print(\"Nessuna regione luminosa rilevata.\")\n",
    "        return None\n",
    "    largest_region = max(regions, key=lambda r: r.area)\n",
    "    minr, minc, maxr, maxc = largest_region.bbox\n",
    "\n",
    "    height = maxr - minr + offset\n",
    "    width = maxc - minc + offset\n",
    "    \n",
    "    side_length = max(height, width)\n",
    "\n",
    "    center_r = (minr + maxr) // 2\n",
    "    center_c = (minc + maxc) // 2\n",
    "\n",
    "    minr = center_r - side_length // 2\n",
    "    maxr = center_r + side_length // 2\n",
    "    minc = center_c - side_length // 2\n",
    "    maxc = center_c + side_length // 2\n",
    "    \n",
    "    if minr < 0:\n",
    "        maxr += abs(minr)  \n",
    "        minr = 0\n",
    "    if maxr > image.shape[0]:\n",
    "        minr -= maxr - image.shape[0]\n",
    "        maxr = image.shape[0]\n",
    "\n",
    "    if minc < 0:\n",
    "        maxc += abs(minc)   \n",
    "        minc = 0\n",
    "    if maxc > image.shape[1]:\n",
    "        minc -= maxc - image.shape[1]\n",
    "        maxc = image.shape[1]\n",
    "\n",
    "    minr = max(0, minr)\n",
    "    minc = max(0, minc)\n",
    "    maxr = min(image.shape[0], maxr)\n",
    "    maxc = min(image.shape[1], maxc)\n",
    "\n",
    "    cropped_image = image[minr:maxr, minc:maxc]\n",
    "\n",
    "\n",
    "\n",
    "    return cropped_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function identifies and isolates the brightest region of an image by performing a square crop centered on it. It uses Otsu's threshold method to determine the brightest pixels and then identifies the largest connected region using regionprops. After calculating an extended bounding box with an offset, it determines a square crop based on the longest side of the detected area, centering the crop on the bright region.\n",
    "\n",
    "If the crop exceeds the boundaries of the image, the edges are corrected to keep the crop within the image itself. In the absence of bright regions, the function returns None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_img = detect_eclipse(nrm_img)\n",
    "\n",
    "\n",
    "#Print Histogram\n",
    "hist, bins = np.histogram(nrm_img.flatten(), bins=256, range=(0, 1))\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.title(\"Istogramma delle Intensità\")\n",
    "plt.bar(bins[:-1], hist, width=0.005, color='gray')\n",
    "plt.xlabel(\"Intensità\")\n",
    "plt.ylabel(\"Numero di pixel\")\n",
    "plt.show()\n",
    "\n",
    "#Print Cropped Img\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Immagine Ritagliata\")\n",
    "plt.imshow(cropped_img, cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downscaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "\n",
    "def downscale(\n",
    "    img: np.ndarray,\n",
    "    target_resolution: tuple = (512, 512),\n",
    "    sigma: float = 1\n",
    ") -> np.ndarray:\n",
    "\n",
    "    if img.ndim == 3:   \n",
    "        img_blurred = np.zeros_like(img)\n",
    "        for c in range(3):\n",
    "            img_blurred[:, :, c] = gaussfiltering(img[:, :, c], sigma)\n",
    "    else:   \n",
    "        img_blurred = gaussfiltering(img, sigma)\n",
    "\n",
    "    downscaled_img = resize(img_blurred, target_resolution, anti_aliasing=True)\n",
    "\n",
    "    if img.ndim == 3:  \n",
    "        downscaled_img = np.clip(downscaled_img * 255, 0, 255).astype(np.uint8)\n",
    "        img_blurred = np.clip(img_blurred * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "    return downscaled_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downscale function downscales an image to a specified target resolution, applying Gaussian pre-filtering to reduce artifacts due to sampling.\n",
    "\n",
    "Main details:\n",
    "\n",
    "* Accepts an input image (img) and an optional target resolution parameter (target_resolution, default (512, 512)).\n",
    "* Applies Gaussian filtering (gaussfiltering) to smooth the image before resizing.\n",
    "* Uses skimage.transform.resize to resize the image, with anti_aliasing option to preserve visual quality.\n",
    "* Converts filtered and resized images to 8-bit (uint8) format to ensure compatibility and optimize memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downscaled = downscale(cropped_img, target_resolution=(512, 512), sigma=3)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 8))\n",
    "plt.gray()\n",
    "fig.tight_layout()\n",
    "\n",
    "axes[0].imshow(cropped_img)\n",
    "axes[0].set_title(f\"Original Image {smt_img.shape}\")\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(downscaled)\n",
    "axes[1].set_title(f\"Downscaled Image {downscaled.shape}\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(path: str, target_resolution: tuple = (512, 512), sigma: float = 3, offset: int = 128):\n",
    "    image = cv2.imread(path)\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    smt_img = gaussfiltering(gray_image, sigma)\n",
    "    nrm_img = normalize(smt_img)\n",
    "    cropped_img = detect_eclipse(nrm_img)\n",
    "    downscaled_img = downscale(cropped_img, target_resolution, sigma)\n",
    "    return downscaled_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def get_image_paths(folder_path):\n",
    "    supported_formats = ('.jpg', '.png')\n",
    "    return [\n",
    "        os.path.join(folder_path, filename)\n",
    "        for filename in os.listdir(folder_path)\n",
    "        if filename.lower().endswith(supported_formats)\n",
    "    ]\n",
    "\n",
    "def preprocess_images_in_parallel(folder_path, target_resolution=(512, 512), sigma=3, num_workers=4):\n",
    "    image_paths = get_image_paths(folder_path)\n",
    "    \n",
    "    def process(path):\n",
    "        return preprocess_image(path, target_resolution, sigma)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        processed_images = list(executor.map(process, image_paths))\n",
    "    \n",
    "    return np.array(processed_images)\n",
    "\n",
    "def save_to_npy(dataset, output_file):\n",
    "    np.save(output_file, dataset)\n",
    "\n",
    "input_folder = \"eclipse-megamovie/train\"\n",
    "output_file = \"/Users/valeriosantini/Desktop/preprocessed_dataset.npy\"\n",
    "target_resolution = (512, 512)\n",
    "sigma = 3 \n",
    "\n",
    "processed_images = preprocess_images_in_parallel(input_folder, target_resolution, sigma, num_workers=8)\n",
    "save_to_npy(processed_images, output_file)\n",
    "\n",
    "print(f\"Preprocessing completato e dataset salvato in {output_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il file .npy\n",
    "data = np.load(\"/Users/valeriosantini/Desktop/preprocessed_dataset.npy\")\n",
    "\n",
    "# Stampa alcune informazioni\n",
    "print(\"Shape:\", data.shape)\n",
    "print(\"Data type:\", data.dtype)\n",
    "\n",
    "# Mostra un esempio\n",
    "print(\"Primo elemento (array):\")\n",
    "print(data[0])  # Mostra il primo elemento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# Dataset personalizzato\n",
    "class SolarEclipseDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_folder, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)  # Carica il file CSV\n",
    "        self.image_folder = image_folder\n",
    "        self.target_resolution = target_resolution\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_folder, self.data.iloc[idx, 0])\n",
    "        image = cv2.imread(img_name, cv2.IMREAD_GRAYSCALE)  # Carica in grayscale\n",
    "        image = cv2.resize(image, self.target_resolution)  # Ridimensiona l'immagine\n",
    "        label = self.data.iloc[idx, 1]\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Trasformazioni (normalizzazione e resize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converte l'immagine in tensore\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalizzazione\n",
    "])\n",
    "\n",
    "# Carica il dataset\n",
    "csv_file = \"eclipse-megamovie/train.csv\"\n",
    "image_folder = \"eclipse-megamovie/train\"\n",
    "dataset = SolarEclipseDataset(csv_file, image_folder, transform=transform)\n",
    "\n",
    "# DataLoader per batch\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SolarEclipseCNN(nn.Module):\n",
    "    def __init__(self, num_classes=8):\n",
    "        super(SolarEclipseCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)  # Input: 1x512x512 -> Output: 32x512x512\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)  # Output: 64x512x512\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Riduce dimensione: Output: 64x256x256\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)  # Output: 128x256x256\n",
    "        self.fc1 = nn.Linear(128 * 64 * 64, 512)  # Flatten -> Fully connected\n",
    "        self.fc2 = nn.Linear(512, num_classes)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # Output: 128x64x64\n",
    "        x = x.view(-1, 128 * 64 * 64)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Istanziamento del modello, loss e ottimizzatore\n",
    "#device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\")\n",
    "model = SolarEclipseCNN(num_classes=8).to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # Loss per classificazione multiclass\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Loop di training\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass e ottimizzazione\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "print(\"Training completato.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model \n",
    "model_path = \"eclipse-megamovie/solar_net.pth\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Modalità valutazione\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Report di classificazione\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Matrice di confusione\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try preprocessing with an image given its path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, UnidentifiedImageError\n",
    "\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.image_names = [\n",
    "            f for f in os.listdir(image_folder) \n",
    "            if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))\n",
    "        ]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_names[idx]\n",
    "        img_path = os.path.join(self.image_folder, img_name)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"L\")  # Converti in scala di grigi\n",
    "        except UnidentifiedImageError:\n",
    "            raise ValueError(f\"Il file {img_path} non è un'immagine valida.\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, img_name\n",
    "\n",
    "    \n",
    "# Definisci le trasformazioni\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # Ridimensiona le immagini\n",
    "    transforms.ToTensor(),  # Converti in tensore\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalizza\n",
    "])\n",
    "\n",
    "# Carica il dataset\n",
    "image_folder = \"eclipse-megamovie/test\"  # Percorso alla tua cartella di test\n",
    "test_dataset = CustomTestDataset(image_folder=image_folder, transform=transform)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Metti il modello in modalità di valutazione\n",
    "model.eval()\n",
    "\n",
    "# Predizioni\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, img_names = data\n",
    "        images = images.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Predici le classi\n",
    "\n",
    "        # Salva i risultati\n",
    "        for img_name, label in zip(img_names, predicted.cpu().numpy()):\n",
    "            predictions.append((img_name, label))\n",
    "\n",
    "# Stampa l'accuratezza (se hai etichette vere, puoi calcolarla)\n",
    "print(\"Predizioni completate.\")\n",
    "\n",
    "# Salva i risultati in un CSV\n",
    "df = pd.DataFrame(predictions, columns=[\"Image_Name\", \"Predicted_Label\"])\n",
    "df.to_csv(\"eclipse_predictions.csv\", index=False)\n",
    "print(\"Predizioni salvate in: eclipse_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_folder = \"eclipse-megamovie/train\"\n",
    "# prcs_img = []\n",
    "\n",
    "# for filename in os.listdir(img_folder):\n",
    "#     if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "#         image_path = os.path.join(img_folder, filename)\n",
    "#         processed_image = preprocess_image(image_path)\n",
    "#         #print(filename)\n",
    "#         image_255 = (processed_image * 255).astype(np.uint8)\n",
    "#         cv2.imwrite(f'/Users/valeriosantini/Desktop/provapre/{filename}', image_255)\n",
    "\n",
    "preprocessed_img = preprocess_image(\"eclipse-megamovie/train/0017d87ef1aeb1cc9003d013e652885755657aef55c22c248da595a3d480b119.jpg\")\n",
    "\n",
    "plt.imshow(preprocessed_img)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Estrazione delle feature\n",
    "\n",
    "-   **Istogrammi:Spiegazioni:** Spiegare come gli istogrammi possono essere utilizzati per estrarre feature dalle immagini di eclissi solare. [1]\n",
    "-   Descrivere come la distribuzione dei pixel nell'istogramma può variare a seconda della fase dell'eclissi.\n",
    "-   **Codice:** Fornire il codice Python utilizzato per calcolare gli istogrammi.\n",
    "-   **Visualizzazioni:** Mostrare alcuni esempi di istogrammi per le diverse fasi dell'eclissi.\n",
    "-   **Altre tecniche (opzionale):** Se si utilizzano altre tecniche di estrazione delle feature, includere una sezione simile a quella degli istogrammi, spiegando e visualizzando i risultati.\n",
    "\n",
    "### Suggerimento su come procedere\n",
    "\n",
    "Ecco i passaggi da eseguire, considerando che hai già le funzioni di pre-elaborazione e hai ottenuto immagini quadrate e ridimensionate del sole centrato:\n",
    "\n",
    "*   **Organizzare il dataset:** Crea una struttura per il tuo dataset, ad esempio un dizionario o un dataframe Pandas, dove ogni elemento contiene:\n",
    "    *   Percorso dell'immagine\n",
    "    *   Etichetta della fase dell'eclissi (es. \"parziale\", \"totale\", \"anulare\")\n",
    "*   **Applicare le funzioni di pre-elaborazione:** Per ogni immagine nel dataset:\n",
    "    *   Carica l'immagine.\n",
    "    *   Applica le funzioni di pre-elaborazione esistenti (conversione in scala di grigi, filtro gaussiano, normalizzazione, rilevamento e ritaglio dell'eclissi, ridimensionamento).\n",
    "*   **Calcolare l'istogramma:** Per ogni immagine pre-elaborata:\n",
    "    *   Calcola l'istogramma dell'immagine utilizzando una libreria come Matplotlib o OpenCV.\n",
    "    *   Aggiungi l'istogramma al dataset, associandolo all'immagine corrispondente.\n",
    "*   **Addestrare il modello:** Utilizza il set di training, composto dagli istogrammi e dalle relative etichette, per addestrare il modello di classificazione.\n",
    "*   **Valutare il modello:** Utilizza il set di test per valutare le prestazioni del modello addestrato.\n",
    "\n",
    "Ricorda che puoi anche esplorare l'utilizzo di altre tecniche di estrazione delle feature, come **Edge Detection**, per ottenere feature aggiuntive e migliorare potenzialmente l'accuratezza del modello.  Le fonti che hai fornito offrono informazioni dettagliate su queste tecniche e sulla loro implementazione.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scelta e implementazione del modello\n",
    "\n",
    "-   **Modello scelto:** Indicare chiaramente il modello di classificazione scelto per il progetto (ad esempio, Regressione Logistica, Regressione Softmax, CNN). [4-6]\n",
    "-   Giustificare la scelta del modello in base alle caratteristiche del problema e del dataset.\n",
    "-   Fornire una breve descrizione teorica del modello scelto, evidenziandone i principi di funzionamento.\n",
    "-   **Codice:** Fornire il codice Python utilizzato per implementare il modello.\n",
    "-   Utilizzare le competenze acquisite durante la realizzazione di \"HomeWork.pdf\" per implementare il modello in modo efficiente. [7]\n",
    "-   Commentare il codice in modo chiaro per renderlo comprensibile.\n",
    "-   **Training del modello:Spiegazioni:** Descrivere la procedura di training del modello, specificando la funzione di loss, l'algoritmo di ottimizzazione e gli iperparametri utilizzati. [8-20]\n",
    "-   Spiegare come si è divisa il dataset in set di training e di test.\n",
    "-   **Codice:** Fornire il codice Python utilizzato per il training del modello.\n",
    "-   **Visualizzazioni:** Mostrare l'andamento della funzione di loss durante il training. [21-27]\n",
    "-   Plottare anche l'accuratezza sul set di training e sul set di test durante il training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Valutazione del modello\n",
    "\n",
    "-   **Spiegazioni:** Descrivere le metriche utilizzate per valutare le prestazioni del modello. [13, 28-32]\n",
    "-   Oltre all'accuratezza, considerare altre metriche come precisione, recall e F1-score, soprattutto se si ha uno sbilanciamento di classe.\n",
    "-   **Codice:** Fornire il codice Python utilizzato per calcolare le metriche di valutazione.\n",
    "-   **Risultati:** Presentare i risultati ottenuti dal modello sul set di test.\n",
    "-   Utilizzare tabelle e grafici per visualizzare i risultati in modo chiaro ed efficace.\n",
    "-   **Discussione:** Analizzare i risultati ottenuti, commentando le prestazioni del modello e le eventuali limitazioni."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Video delle transizioni dell'eclissi (opzionale) \n",
    "\n",
    "-   **Spiegazioni:** Descrivere il processo di creazione del video, includendo il software utilizzato e le scelte creative fatte.\n",
    "-   Spiegare come si sono selezionate le immagini per il video e come si è creata la sequenza di transizione.\n",
    "-   **Video:** Includere il video nel notebook Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusioni\n",
    "\n",
    "-   **Riepilogo:** Riassumere brevemente il lavoro svolto nel progetto.\n",
    "-   **Risultati principali:** Evidenziare i risultati più importanti ottenuti.\n",
    "-   **Sviluppi futuri:** Suggerire eventuali miglioramenti o sviluppi futuri del progetto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
